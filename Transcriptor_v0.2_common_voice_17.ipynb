{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1JJmzHXYF_LQQC-JTZWJkmTe7ywaAqpKF",
      "authorship_tag": "ABX9TyP4WgCiAYrG7PX7HGpw3uR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leafanine/Custom_Language_TL/blob/master/Transcriptor_v0.2_common_voice_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Odia data from Mozilla Common Voice\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"or\", split=\"train\", trust_remote_code=True)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "print(dataset[\"train\"][0])\n",
        "\n",
        "\n",
        "print(\"__Success__\")\n",
        "print(\"\\n\")\n",
        "print(\"__TRAIN SPLIT__\\n\")\n",
        "print(dataset[\"train\"][0])\n",
        "print(\"\\n\")\n",
        "print(\"__TEST SPLIT__\\n\")\n",
        "print(dataset[\"test\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOFRvbNAyrpv",
        "outputId": "5ed3d220-3c95-475b-f2c5-923ee74e6967"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'client_id': 'b0387daa800984e4069a5b7ee53956cb1c5f8d5a876ca0b4072260237d47d74afe21ec0267f9d9076c438d1af441fa9fada6c3a0bb2f10251b5b2ced597c5879', 'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22624077.mp3', 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22624077.mp3', 'array': array([ 0.00000000e+00,  1.52326678e-15,  2.12415090e-15, ...,\n",
            "       -8.73520548e-06, -1.69834202e-05, -2.99184849e-05]), 'sampling_rate': 48000}, 'sentence': '‡¨≠‡¨æ‡¨£‡≠ç‡¨°‡¨∂‡≠Ç‡¨®‡≠ç‡≠ü ‡¨π‡≠á‡¨¨‡¨æ ‡¨ï‡¨æ‡¨∞‡≠ç‡¨Ø‡≠ç‡≠ü‡¨ü‡¨æ ‡¨Ø‡≠á ‡¨Æ‡¨ô‡≠ç‡¨ó‡¨∞‡¨æ‡¨ú‡¨ô‡≠ç‡¨ï ‡¨¶‡≠ç‡¨µ‡¨æ‡¨∞‡¨æ ‡¨Ö‡¨®‡≠Å‡¨∑‡≠ç‡¨†‡¨ø‡¨§ ‡¨π‡≠ã‡¨á‡¨Ö‡¨õ‡¨ø, ‡¨è‡¨•‡¨ø‡¨∞ ‡¨ö‡¨æ‡¨ï‡≠ç‡¨∑‡≠Å‡¨∑ ‡¨∏‡¨æ‡¨ï‡≠ç‡¨∑‡≠Ä ‡¨ï‡¨æ‡¨π‡¨ø‡¨Å?', 'up_votes': 3, 'down_votes': 0, 'age': 'thirties', 'gender': 'male_masculine', 'accent': 'Central,Baleswari', 'locale': 'or', 'segment': '', 'variant': ''}\n",
            "__Success__\n",
            "\n",
            "\n",
            "__TRAIN SPLIT__\n",
            "\n",
            "{'client_id': 'b0387daa800984e4069a5b7ee53956cb1c5f8d5a876ca0b4072260237d47d74afe21ec0267f9d9076c438d1af441fa9fada6c3a0bb2f10251b5b2ced597c5879', 'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22624077.mp3', 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22624077.mp3', 'array': array([ 0.00000000e+00,  1.52326678e-15,  2.12415090e-15, ...,\n",
            "       -8.73520548e-06, -1.69834202e-05, -2.99184849e-05]), 'sampling_rate': 48000}, 'sentence': '‡¨≠‡¨æ‡¨£‡≠ç‡¨°‡¨∂‡≠Ç‡¨®‡≠ç‡≠ü ‡¨π‡≠á‡¨¨‡¨æ ‡¨ï‡¨æ‡¨∞‡≠ç‡¨Ø‡≠ç‡≠ü‡¨ü‡¨æ ‡¨Ø‡≠á ‡¨Æ‡¨ô‡≠ç‡¨ó‡¨∞‡¨æ‡¨ú‡¨ô‡≠ç‡¨ï ‡¨¶‡≠ç‡¨µ‡¨æ‡¨∞‡¨æ ‡¨Ö‡¨®‡≠Å‡¨∑‡≠ç‡¨†‡¨ø‡¨§ ‡¨π‡≠ã‡¨á‡¨Ö‡¨õ‡¨ø, ‡¨è‡¨•‡¨ø‡¨∞ ‡¨ö‡¨æ‡¨ï‡≠ç‡¨∑‡≠Å‡¨∑ ‡¨∏‡¨æ‡¨ï‡≠ç‡¨∑‡≠Ä ‡¨ï‡¨æ‡¨π‡¨ø‡¨Å?', 'up_votes': 3, 'down_votes': 0, 'age': 'thirties', 'gender': 'male_masculine', 'accent': 'Central,Baleswari', 'locale': 'or', 'segment': '', 'variant': ''}\n",
            "\n",
            "\n",
            "__TEST SPLIT__\n",
            "\n",
            "{'client_id': 'b0387daa800984e4069a5b7ee53956cb1c5f8d5a876ca0b4072260237d47d74afe21ec0267f9d9076c438d1af441fa9fada6c3a0bb2f10251b5b2ced597c5879', 'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22366949.mp3', 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22366949.mp3', 'array': array([ 0.00000000e+00, -1.27581431e-13, -2.35349934e-13, ...,\n",
            "        3.40413453e-06, -2.36611800e-06, -3.94917970e-06]), 'sampling_rate': 48000}, 'sentence': '‡¨®‡¨æ‡¨≤‡≠Å ‡¨ì ‡¨ú‡≠ç‡≠ü‡≠ã‡¨§‡¨ø‡¨∑ ‡¨Æ‡¨ß‡≠ç‡≠ü‡¨∞‡≠á ‡¨è‡¨π‡¨ø‡¨™‡¨∞‡¨ø ‡¨ï‡¨•‡≠ã‡¨™‡¨ï‡¨•‡¨® ‡¨π‡≠á‡¨≤‡¨æ ‡•§', 'up_votes': 3, 'down_votes': 0, 'age': 'thirties', 'gender': 'male_masculine', 'accent': 'Central,Baleswari', 'locale': 'or', 'segment': '', 'variant': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-trCXSRDzCpl",
        "outputId": "ba11b352-ec5e-4c1e-878c-1962ff5ffa15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "print(sf.__libsndfile_version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC3QBczFzvpf",
        "outputId": "1579fa77-9ed5-4d9d-fdae-52800f2aab5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "# Resample audio to 16kHz for both train and test\n",
        "dataset[\"train\"] = dataset[\"train\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "dataset[\"test\"] = dataset[\"test\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "# Rename \"sentence\" to \"text\" (guarded in case it's not present)\n",
        "if \"sentence\" in dataset[\"train\"].column_names:\n",
        "    dataset[\"train\"] = dataset[\"train\"].rename_column(\"sentence\", \"text\")\n",
        "    dataset[\"test\"] = dataset[\"test\"].rename_column(\"sentence\", \"text\")\n",
        "\n",
        "# Remove unused columns\n",
        "keep_cols = [\"audio\", \"text\"]\n",
        "dataset[\"train\"] = dataset[\"train\"].remove_columns([col for col in dataset[\"train\"].column_names if col not in keep_cols])\n",
        "dataset[\"test\"] = dataset[\"test\"].remove_columns([col for col in dataset[\"test\"].column_names if col not in keep_cols])\n",
        "\n",
        "# Print sample\n",
        "print(dataset[\"train\"][0])\n",
        "print(\"‚úÖ Process over\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no2suhDozxl5",
        "outputId": "f34625e7-02b2-42eb-b6db-56247a3ca94b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/54e17a323906a6944843f82586dad3f9c044812f11cc75f29550578584dbb3d8/or_train_0/common_voice_or_22624077.mp3', 'array': array([ 2.72848411e-12,  9.09494702e-12, -2.72848411e-12, ...,\n",
            "       -5.31176056e-06,  1.51929999e-06, -5.82979601e-06]), 'sampling_rate': 16000}, 'text': '‡¨≠‡¨æ‡¨£‡≠ç‡¨°‡¨∂‡≠Ç‡¨®‡≠ç‡≠ü ‡¨π‡≠á‡¨¨‡¨æ ‡¨ï‡¨æ‡¨∞‡≠ç‡¨Ø‡≠ç‡≠ü‡¨ü‡¨æ ‡¨Ø‡≠á ‡¨Æ‡¨ô‡≠ç‡¨ó‡¨∞‡¨æ‡¨ú‡¨ô‡≠ç‡¨ï ‡¨¶‡≠ç‡¨µ‡¨æ‡¨∞‡¨æ ‡¨Ö‡¨®‡≠Å‡¨∑‡≠ç‡¨†‡¨ø‡¨§ ‡¨π‡≠ã‡¨á‡¨Ö‡¨õ‡¨ø, ‡¨è‡¨•‡¨ø‡¨∞ ‡¨ö‡¨æ‡¨ï‡≠ç‡¨∑‡≠Å‡¨∑ ‡¨∏‡¨æ‡¨ï‡≠ç‡¨∑‡≠Ä ‡¨ï‡¨æ‡¨π‡¨ø‡¨Å?'}\n",
            "‚úÖ Process over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Extract all text at once from training dataset\n",
        "all_text = \" \".join(example[\"text\"] for example in dataset[\"train\"])\n",
        "\n",
        "# 2. Keep only Odia characters and spaces\n",
        "all_text = re.sub(r\"[^\\u0B00-\\u0B7F\\s]\", \"\", all_text)\n",
        "\n",
        "# 3. Replace space with '|' later, remove it now\n",
        "char_list = list(all_text.replace(\" \", \"\"))\n",
        "\n",
        "# 4. Count frequencies (optional)\n",
        "vocab_counter = Counter(char_list)\n",
        "\n",
        "# 5. Build vocab dictionary\n",
        "vocab_dict = {char: idx for idx, char in enumerate(sorted(vocab_counter))}\n",
        "vocab_dict[\"|\"] = len(vocab_dict)  # Special token for space\n",
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "\n",
        "# 6. Save vocabulary\n",
        "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vocab_dict, f, ensure_ascii=False)\n",
        "\n",
        "# 7. Output\n",
        "print(vocab_dict)\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcdmeTYIzyx1",
        "outputId": "4c571a87-352a-48ec-cf34-6e23a3c9332c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'‡¨Å': 0, '‡¨Ç': 1, '‡¨É': 2, '‡¨Ö': 3, '‡¨Ü': 4, '‡¨á': 5, '‡¨à': 6, '‡¨â': 7, '‡¨ä': 8, '‡¨ã': 9, '‡¨è': 10, '‡¨ì': 11, '‡¨î': 12, '‡¨ï': 13, '‡¨ñ': 14, '‡¨ó': 15, '‡¨ò': 16, '‡¨ô': 17, '‡¨ö': 18, '‡¨õ': 19, '‡¨ú': 20, '‡¨ù': 21, '‡¨û': 22, '‡¨ü': 23, '‡¨†': 24, '‡¨°': 25, '‡¨¢': 26, '‡¨£': 27, '‡¨§': 28, '‡¨•': 29, '‡¨¶': 30, '‡¨ß': 31, '‡¨®': 32, '‡¨™': 33, '‡¨´': 34, '‡¨¨': 35, '‡¨≠': 36, '‡¨Æ': 37, '‡¨Ø': 38, '‡¨∞': 39, '‡¨≤': 40, '‡¨≥': 41, '‡¨µ': 42, '‡¨∂': 43, '‡¨∑': 44, '‡¨∏': 45, '‡¨π': 46, '‡¨º': 47, '‡¨æ': 48, '‡¨ø': 49, '‡≠Ä': 50, '‡≠Å': 51, '‡≠Ç': 52, '‡≠É': 53, '‡≠á': 54, '‡≠à': 55, '‡≠ã': 56, '‡≠å': 57, '‡≠ç': 58, '‡≠ü': 59, '‡≠±': 60, '|': 61, '[UNK]': 62, '[PAD]': 63}\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    \"vocab.json\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    word_delimiter_token=\"|\"\n",
        ")\n",
        "\n",
        "# Save tokenizer for later use\n",
        "tokenizer.save_pretrained(\"odia_tokenizer\")\n",
        "\n",
        "print(\"‚úÖ Tokenizer created and saved as 'odia_tokenizer/'\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VJzV6Nxz0Nn",
        "outputId": "77b8c405-6d79-4503-e01b-7371a88c269c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenizer created and saved as 'odia_tokenizer/'\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Initialize feature extractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\n",
        "    feature_size=1,\n",
        "    sampling_rate=16000,\n",
        "    padding_value=0.0,\n",
        "    do_normalize=True,\n",
        "    return_attention_mask=True\n",
        ")\n",
        "\n",
        "# Save feature extractor\n",
        "feature_extractor.save_pretrained(\"odia_feature_extractor\")\n",
        "\n",
        "print(\"‚úÖ Feature extractor created and saved as 'odia_feature_extractor/'\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3PYBuVpz2He",
        "outputId": "82da7653-8b35-4458-ca29-1fd7ac4f3614"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature extractor created and saved as 'odia_feature_extractor/'\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "# Load tokenizer (from vocab JSON)\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    \"vocab.json\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    word_delimiter_token=\"|\"\n",
        ")\n",
        "\n",
        "# Combine tokenizer + feature extractor\n",
        "processor = Wav2Vec2Processor(\n",
        "    feature_extractor=feature_extractor,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Save the processor\n",
        "processor.save_pretrained(\"odia_processor\")\n",
        "\n",
        "print(\"‚úÖ Processor saved as 'odia_processor/'\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzJErA1Nz3hG",
        "outputId": "93bcef7c-8f11-43c3-fd6e-ed815d90c701"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processor saved as 'odia_processor/'\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to process each example\n",
        "def prepare_dataset(batch):\n",
        "    # Extract raw audio and sampling rate\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # Apply feature extractor ONLY on audio\n",
        "    batch[\"input_values\"] = processor.feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_values[0]\n",
        "\n",
        "    # Ensure text is a string\n",
        "    text = batch.get(\"text\", \"\")\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text) if text is not None else \"\"\n",
        "\n",
        "    # Tokenize transcript ONLY using tokenizer\n",
        "    batch[\"labels\"] = processor.tokenizer(text).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing separately to train and test splits\n",
        "dataset[\"train\"] = dataset[\"train\"].map(prepare_dataset, remove_columns=[\"audio\", \"text\"])\n",
        "dataset[\"test\"] = dataset[\"test\"].map(prepare_dataset, remove_columns=[\"audio\", \"text\"])\n",
        "\n",
        "# Set format for PyTorch\n",
        "dataset[\"train\"].set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
        "dataset[\"test\"].set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
        "\n",
        "print(\"‚úÖ Dataset preprocessing complete\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "dd230f4c4b1d4946aab17b1496937505",
            "abc5f502015c400b9e8718889c75ab12",
            "cc5edd56d6a64a56adedd2b4bde0de1e",
            "e3bd323e4aa44dde97e5ea1e42a8646e",
            "01e62c3f5f2e47c880ef435b224f9d2b",
            "52e1ab46d43f414e956fdbcaa38981b6",
            "88d255b6f1e947e69944c25d7e0b4efd",
            "1775b8e5b3c9447c9aa658e8c590f0f4",
            "36a9239f8c8149aba8dc999171fc1286",
            "e1749d1cc15a462984a95bdc7908bfd4",
            "0b555853235f48dab1824a30686f4d05",
            "95f967740c2640e58d71a6c7632188b0",
            "ccbba7a0380d408a9d88b4b826d5888e",
            "4828fecfc4734040b75aa079e59871c4",
            "766ab23e6b914b38850208be1c821cc7",
            "1b2584f072bc4d32adabd09eb01f6f6a",
            "a1e6afd55085485a8b90b74efdeda9b2",
            "9f816d82e21d4804831cb59730e5f767",
            "db333a29ad2d44c1826c18941ab5b748",
            "f24a3a9b4ba848909b1976e87be9e82c",
            "19cf046510304892acf80957d2e71499",
            "d58f5fb82bb04ea3910492ef60c6c846"
          ]
        },
        "id": "S0nDFv3Vz4jg",
        "outputId": "5b1bbe5a-c069-4f3e-c236-af1809a27167"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd230f4c4b1d4946aab17b1496937505"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95f967740c2640e58d71a6c7632188b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset preprocessing complete\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    vocab_size=len(processor.tokenizer),\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id\n",
        ")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210,
          "referenced_widgets": [
            "c9cb55c1213a490193211933a0f6580b",
            "9eda09cac7064c389c4a35fa380ea341",
            "8bf026a8f50f40d1b3f7b6d0b65094ab",
            "b9bb4528bc024564829a27539e9b72f1",
            "a354012e29ce44dfa290939724854010",
            "8aabc70e77144b30a49261620ace27e6",
            "cbc3e21c5f2943548f686f8854359988",
            "1f5b55c350f44342a26fcadb28b39499",
            "c93f65fb783b48fc8f8020a728a2d7f9",
            "a6757260602140399a8eb21539404694",
            "632fa2ebc2944a81afb841b88992e559",
            "ba540010f5844cf78cf0f86ed3f91696",
            "907a313649524e4c92f1d22bb924a83a",
            "73e221a149f84c7594f8818fbe8048dd",
            "d6ba03dcad11411d87ccd41ac26e4814",
            "0c873009d58940619eb548a5b5518e00",
            "1b359e62559b4101b1f30d01606c4475",
            "5b131ce0adfb43d2954552fc9cb50bce",
            "6cd14db334eb4e51aa50f523eba8d1be",
            "8c00df332e7a4416ba5cc2f7fede3c82",
            "b3f45cc13a434660901746e1510ef4a9",
            "2dd1c787dfb5496d953b831d82d2f508"
          ]
        },
        "id": "7tbysVBGz7D9",
        "outputId": "4986b99d-0ada-49c4-b6ed-b3fd40389fe7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9cb55c1213a490193211933a0f6580b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba540010f5844cf78cf0f86ed3f91696"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUt7rvOoz-GD",
        "outputId": "3395fb41-ad1a-4404-c1af-2a4b297ea243"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n",
            "2.6.0+cu124\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2-odia\",\n",
        "    group_by_length=True,  # Helps with efficient batching for variable-length inputs\n",
        "    per_device_train_batch_size=16,  # You can likely handle 4‚Äì8 depending on sequence length\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 8\n",
        "    num_train_epochs=15,\n",
        "    fp16=True,  # T4 supports mixed precision well\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=400,\n",
        "    save_steps=400,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=1000,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "B6OrEQmkz__c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DataCollatorCTCWithPadding:\n",
        "    def __init__(self, processor, padding=True):\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, features):\n",
        "        input_values = [f[\"input_values\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            {\"input_values\": input_values},\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            {\"input_ids\": labels},\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Replace padding with -100 to ignore in loss\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtB9_GeD0B48",
        "outputId": "00d2ab7a-56eb-450e-d71b-cd9686929042"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Split dataset\n",
        "# Corrected version\n",
        "ds = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "print(ds)\n",
        "\n",
        "training_args.group_by_length = False\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"test\"],\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujtx08Fh0DWr",
        "outputId": "36c2ec73-0e6b-4f5a-da0f-d935d791ac20"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_values', 'labels'],\n",
            "        num_rows: 1658\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_values', 'labels'],\n",
            "        num_rows: 185\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-866ae2abc002>:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8rquxp20GJl",
        "outputId": "f7d0f551-9b4a-4793-a86e-41a6582836fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
        "print(\"Device in use: \", torch.cuda.current_device())\n",
        "print(\"Device name: \", torch.cuda.get_device_name(0))\n",
        "print(\"Memory allocated: \", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
        "print(\"___STARTING_TRAINING___\")\n",
        "trainer.train()\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "IFC0TDjv0IE7",
        "outputId": "a9e9cf03-bf53-4227-aebb-00f655ca9ebd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available?  True\n",
            "Device in use:  0\n",
            "Device name:  Tesla T4\n",
            "Memory allocated:  360.443359375 MB\n",
            "___STARTING_TRAINING___\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [780/780 20:56, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.454500</td>\n",
              "      <td>3.381790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./drive/MyDrive/wav2vec2-odia-17.0\")\n",
        "tokenizer.save_pretrained(\"./drive/MyDrive/wav2vec2-odia-17.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiK6f_JB0Jhe",
        "outputId": "9496b9ff-6d6e-4b03-a2b2-b826af2feb75"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./drive/MyDrive/wav2vec2-odia-17.0/tokenizer_config.json',\n",
              " './drive/MyDrive/wav2vec2-odia-17.0/special_tokens_map.json',\n",
              " './drive/MyDrive/wav2vec2-odia-17.0/vocab.json',\n",
              " './drive/MyDrive/wav2vec2-odia-17.0/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(\"./drive/MyDrive/wav2vec2-odia-17.0\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ8p6ALy2T19",
        "outputId": "342a632b-c49c-40d0-ea33-db7f7b06f442"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"./drive/MyDrive/wav2vec2-odia-17.0\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"./drive/MyDrive/wav2vec2-odia-17.0\")\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYQmTBCx2UvY",
        "outputId": "2a8a9d9c-2e47-4163-f49d-f151f6ad6698"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = dataset[\"test\"]\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLl1DmAk2Vyw",
        "outputId": "0cf43c8f-7798-440d-be22-f7dc0a40c766"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwAnpwe72Wnk",
        "outputId": "a81aa6f4-582c-4bee-e497-b481da910c1c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def evaluate_model(model, processor, dataset):\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for sample in dataset:\n",
        "        input_values = sample[\"input_values\"].unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_values).logits\n",
        "\n",
        "        pred_ids = torch.argmax(logits, dim=-1)\n",
        "        predicted_text = processor.batch_decode(pred_ids)[0]\n",
        "\n",
        "        # Decode labels (convert token IDs back to text)\n",
        "        label_ids = sample[\"labels\"]\n",
        "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "        reference_text = processor.batch_decode(label_ids.unsqueeze(0))[0]\n",
        "\n",
        "        predictions.append(predicted_text.lower())\n",
        "        references.append(reference_text.lower())\n",
        "\n",
        "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
        "    print(f\"Word Error Rate (WER): {wer:.4f}\")\n",
        "evaluate_model(model, processor, dataset[\"test\"])\n",
        "print(\"\\n_____ PROCESS_OVER _____\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "383189fa9faf4a07b0986a71cc5c6bea",
            "0245ea654b374fdf8623a089b3b04d4e",
            "9d58b64f914749538f467f78417753cf",
            "59c3cbaca322451db12637b8efdca582",
            "7c13724d1dab4aab8f6e80cba00a1f97",
            "44759087461d47df816b879ab717be20",
            "a261bcb606cd4f74960a51ed19098436",
            "9f34a4eba90240e4a19bdbd7d4d4ae8f",
            "247bf12b6c1a42e88d0a5f2863c46f22",
            "ac9adcf59ae84c22a74e6ada5ec68f70",
            "b6406f7ec3d34927a7f3eb0dab6f2f9a"
          ]
        },
        "id": "Hauee9bt2XZg",
        "outputId": "30f0112c-07d0-4cb9-d24a-974d7c7dea7c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "383189fa9faf4a07b0986a71cc5c6bea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate (WER): 0.5191\n",
            "\n",
            "_____ PROCESS_OVER _____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display, Javascript\n",
        "import soundfile as sf\n",
        "import io\n",
        "import base64\n",
        "from google.colab import output\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "#JS script to record voice in collab\n",
        "RECORD_JS = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = () => resolve(reader.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "\n",
        "var record = async () => {\n",
        "  const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "\n",
        "  await sleep(5000)\n",
        "  recorder.stop()\n",
        "\n",
        "  await new Promise(resolve => recorder.onstop = resolve)\n",
        "  const blob = new Blob(chunks)\n",
        "  const b64 = await b2text(blob)\n",
        "  return b64\n",
        "}\n",
        "\"\"\"\n",
        "display(Javascript(RECORD_JS))\n",
        "\n",
        "#record voice\n",
        "def record_audio(filename=\"my_audio.wav\"):\n",
        "    print(\"üéôÔ∏è Speak now (Recording for 10 seconds)...\")\n",
        "    b64 = output.eval_js(\"record()\")\n",
        "    binary = base64.b64decode(b64.split(',')[1])\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(binary)\n",
        "    print(\"‚úÖ Saved:\", filename)\n",
        "record_audio()\n",
        "\n",
        "# Load fine-tuned model\n",
        "model_path = \"./drive/MyDrive/wav2vec2-odia-17.0\"  # Change if different\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Load recorded audio\n",
        "audio, rate = librosa.load(\"my_audio.wav\", sr=16000)\n",
        "\n",
        "# Tokenize and transcribe\n",
        "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "print(\"üó£Ô∏è Transcription:\", transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "NINTq8Jd6hXH",
        "outputId": "31ecabea-c6a9-4c32-9cee-dc5dbbff9a65"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = () => resolve(reader.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "\n",
              "var record = async () => {\n",
              "  const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "\n",
              "  await sleep(5000)\n",
              "  recorder.stop()\n",
              "\n",
              "  await new Promise(resolve => recorder.onstop = resolve)\n",
              "  const blob = new Blob(chunks)\n",
              "  const b64 = await b2text(blob)\n",
              "  return b64\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéôÔ∏è Speak now (Recording for 10 seconds)...\n",
            "‚úÖ Saved: my_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-25df4e347d9b>:54: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, rate = librosa.load(\"my_audio.wav\", sr=16000)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üó£Ô∏è Transcription: ‡¨∏‡¨®‡≠ç‡¨•‡¨æ ‡¨∏‡¨ï‡¨£ ‡¨∏‡≠ç‡¨®‡≠ç‡¨•‡¨æ ‡¨∞‡¨æ‡≠ç‡¨§‡¨∞‡¨ó‡≠Å‡¨π‡¨ø\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHUDoEW-7YvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
